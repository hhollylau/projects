{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# futcurves — real SR3 data test\n",
    "\n",
    "End-to-end test of the library using **real Databento SR3 data** stored on Google Drive.\n",
    "\n",
    "1. Authenticate Google Drive & download SR3 parquet\n",
    "2. Parse contracts into `meta` and `panel` DataFrames\n",
    "3. Build rolling universe\n",
    "4. Build strip curve + holdings (smoothstep roll)\n",
    "5. Inspect roll blending around a real roll date\n",
    "6. Generate contract-level orders\n",
    "7. Visualise curve, term structure, returns\n",
    "\n",
    "**Before publishing:** remove `credentials.json`, `token.pickle`, and any hardcoded API keys."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import os, io, pickle, re\n",
    "from datetime import datetime\n",
    "\n",
    "from futcurves import (\n",
    "    RollPolicy,\n",
    "    build_rolling_universe,\n",
    "    build_strip_curve,\n",
    "    position_to_contract_orders,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Google Drive — download SR3 parquet\n",
    "\n",
    "This pulls `sr3_curve_daily_3years.parquet` from the `Databento_Data` folder on your Drive.\n",
    "If you already have it locally, skip the download cell."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "from google.auth.transport.requests import Request\nfrom google.oauth2.credentials import Credentials\nfrom google_auth_oauthlib.flow import InstalledAppFlow\nfrom googleapiclient.discovery import build\nfrom googleapiclient.http import MediaIoBaseDownload\n\nSCOPES = [\"https://www.googleapis.com/auth/drive.file\"]\n\n# Credentials live in the XCS224 project directory\n_CRED_DIR = os.path.expanduser(\"~/XCS224/A4/src\")\n_CRED_FILE = os.path.join(_CRED_DIR, \"credentials.json\")\n_TOKEN_FILE = os.path.join(_CRED_DIR, \"token.pickle\")\n\ndef authenticate_google_drive():\n    creds = None\n    if os.path.exists(_TOKEN_FILE):\n        with open(_TOKEN_FILE, \"rb\") as f:\n            creds = pickle.load(f)\n    if not creds or not creds.valid:\n        if creds and creds.expired and creds.refresh_token:\n            creds.refresh(Request())\n        else:\n            flow = InstalledAppFlow.from_client_secrets_file(_CRED_FILE, SCOPES)\n            creds = flow.run_local_server(port=0)\n        with open(_TOKEN_FILE, \"wb\") as f:\n            pickle.dump(creds, f)\n    return build(\"drive\", \"v3\", credentials=creds)\n\n\ndef download_from_drive(service, file_name, folder_name=\"Databento_Data\"):\n    q = f\"name='{folder_name}' and mimeType='application/vnd.google-apps.folder' and trashed=false\"\n    folders = service.files().list(q=q, spaces=\"drive\", fields=\"files(id)\").execute().get(\"files\", [])\n    if not folders:\n        raise FileNotFoundError(f\"Folder '{folder_name}' not found on Drive\")\n    folder_id = folders[0][\"id\"]\n    q = f\"name='{file_name}' and '{folder_id}' in parents and trashed=false\"\n    files = service.files().list(q=q, spaces=\"drive\", fields=\"files(id)\").execute().get(\"files\", [])\n    if not files:\n        raise FileNotFoundError(f\"File '{file_name}' not found in '{folder_name}'\")\n    request = service.files().get_media(fileId=files[0][\"id\"])\n    fh = io.FileIO(file_name, \"wb\")\n    downloader = MediaIoBaseDownload(fh, request)\n    done = False\n    while not done:\n        status, done = downloader.next_chunk()\n    print(f\"Downloaded {file_name}\")\n    return file_name\n\n\ndrive_service = authenticate_google_drive()\nprint(\"Google Drive authenticated\")"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "PARQUET_FILE = \"sr3_curve_daily_3years.parquet\"\n",
    "\n",
    "if not os.path.exists(PARQUET_FILE):\n",
    "    download_from_drive(drive_service, PARQUET_FILE)\n",
    "\n",
    "df_raw = pd.read_parquet(PARQUET_FILE)\n",
    "print(f\"Loaded {len(df_raw):,} rows\")\n",
    "print(f\"Date range: {df_raw.index.min()} to {df_raw.index.max()}\")\n",
    "print(f\"Columns: {df_raw.columns.tolist()}\")\n",
    "print(f\"Unique symbols: {df_raw['symbol'].nunique()}\")\n",
    "df_raw.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Parse into `meta` and `panel`\n",
    "\n",
    "Filter to single-leg quarterly contracts (`SR3[HMUZ]\\d`), build the meta and panel DataFrames that `futcurves` expects."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Filter to single-leg quarterly contracts\n",
    "mask = df_raw[\"symbol\"].str.match(r\"^SR3[HMUZ]\\d$\")\n",
    "df = df_raw[mask].copy()\n",
    "print(f\"After filter: {len(df):,} rows, {df['symbol'].nunique()} contracts\")\n",
    "print(f\"Contracts: {sorted(df['symbol'].unique())}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "MONTH_MAP = {\"H\": 3, \"M\": 6, \"U\": 9, \"Z\": 12}\n\n\ndef sr3_expiry(symbol: str, last_seen: pd.Timestamp) -> pd.Timestamp:\n    \"\"\"Estimate expiry as 3rd Wednesday of the contract month.\n    \n    Disambiguates decade using the last date the contract was seen trading —\n    expiry must be >= that date. Works for any date range including pre-2020.\n    \"\"\"\n    month = MONTH_MAP[symbol[3]]\n    digit = int(symbol[4])\n    year = 2020 + digit\n    # Walk forward by decade until expiry is on or after the last trade date\n    while True:\n        first = pd.Timestamp(year, month, 1)\n        wed_offset = (2 - first.weekday()) % 7\n        third_wed = first + pd.Timedelta(days=wed_offset + 14)\n        if third_wed >= last_seen:\n            return third_wed\n        year += 10\n\n\n# For each contract, find the last date it was seen in the data\nlast_seen_per_contract = (\n    df.assign(date=pd.to_datetime(df.index).normalize())\n    .groupby(\"symbol\")[\"date\"]\n    .max()\n)\nif last_seen_per_contract.dt.tz is not None:\n    last_seen_per_contract = last_seen_per_contract.dt.tz_localize(None)\n\ncontracts = sorted(df[\"symbol\"].unique())\nmeta = pd.DataFrame({\n    \"contract\": contracts,\n    \"expiry\": [sr3_expiry(c, last_seen_per_contract[c]) for c in contracts],\n})\nmeta[\"last_trade_date\"] = meta[\"expiry\"] - pd.tseries.offsets.BDay(2)\nmeta = meta.sort_values(\"expiry\").reset_index(drop=True)\n\nprint(f\"meta: {len(meta)} contracts\")\nprint(f\"Expiry range: {meta['expiry'].min().date()} to {meta['expiry'].max().date()}\")\nmeta"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build panel: ts, contract, price\n",
    "df[\"date\"] = pd.to_datetime(df.index).normalize()\n",
    "if df[\"date\"].dt.tz is not None:\n",
    "    df[\"date\"] = df[\"date\"].dt.tz_localize(None)\n",
    "\n",
    "panel = df[[\"date\", \"symbol\", \"close\"]].rename(\n",
    "    columns={\"date\": \"ts\", \"symbol\": \"contract\", \"close\": \"price\"}\n",
    ").reset_index(drop=True)\n",
    "\n",
    "print(f\"panel: {len(panel):,} rows\")\n",
    "print(f\"Date range: {panel['ts'].min().date()} to {panel['ts'].max().date()}\")\n",
    "panel.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Build the rolling universe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "n_positions = 20\nstart = str(panel[\"ts\"].min().date())\nend = str(panel[\"ts\"].max().date())\n\n# offset_bdays=0: keep contracts in the universe until expiry (matching old notebook)\nuniverse = build_rolling_universe(meta, start, end, n_positions=n_positions, offset_bdays=0)\nprint(f\"universe shape: {universe.shape}\")\nprint(f\"{start} to {end}\")\nuniverse.head(10)"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# When does the front contract roll?\n",
    "front = universe[1]\n",
    "roll_dates = front[front != front.shift()].dropna()\n",
    "print(f\"{len(roll_dates)} front contract rolls:\")\n",
    "for d, c in roll_dates.items():\n",
    "    print(f\"  {d.date()}  -> {c}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Build strip curve + holdings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Default RollPolicy: linear blend over the exact gap between consecutive expiries\nroll_policy = RollPolicy()\n\ncurve_px, holdings = build_strip_curve(\n    panel, universe, meta, n_positions=n_positions, roll_policy=roll_policy\n)\n\nprint(f\"curve_px shape: {curve_px.shape}\")\nprint(f\"NaN cells: {curve_px.isna().sum().sum()}\")\ncurve_px.head(10)"
  },
  {
   "cell_type": "code",
   "source": "# Diagnostic: check for suspicious jumps in the curve\n# A large day-over-day price change at the roll boundary signals a problem\nprice_diff = curve_px.diff().abs()\njump_threshold = 0.5  # flag jumps > 50 ticks\n\nfor pos in [1, 5, 10, 20]:\n    jumps = price_diff[pos][price_diff[pos] > jump_threshold].dropna()\n    if len(jumps) > 0:\n        print(f\"pos {pos}: {len(jumps)} jumps > {jump_threshold}\")\n        for d, v in jumps.head(5).items():\n            h = holdings.get(d, {}).get(pos, {})\n            print(f\"  {d.date()}  delta={v:.4f}  holdings={h}\")\n    else:\n        print(f\"pos {pos}: no jumps > {jump_threshold}\")\n\n# NaN summary per position\nnan_per_pos = curve_px.isna().sum()\nif nan_per_pos.any():\n    print(f\"\\nNaN count per position:\\n{nan_per_pos[nan_per_pos > 0]}\")\nelse:\n    print(\"\\nNo NaN values in curve\")",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Inspect roll blending around a real roll date"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pick the second roll and show holdings for positions 1-5 through the window\n",
    "roll_d = roll_dates.index[min(1, len(roll_dates) - 1)]\n",
    "window = pd.date_range(\n",
    "    roll_d - pd.tseries.offsets.BDay(10),\n",
    "    roll_d + pd.tseries.offsets.BDay(3),\n",
    "    freq=\"B\",\n",
    ")\n",
    "\n",
    "print(f\"Roll window around {roll_d.date()}\\n\")\n",
    "for d in window:\n",
    "    if d not in holdings:\n",
    "        continue\n",
    "    h1 = holdings[d].get(1, {})\n",
    "    parts = \"  \".join(f\"{c}:{w:.3f}\" for c, w in h1.items())\n",
    "    print(f\"  {d.date()}  pos1: {parts}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Blend weights table for position 1\n",
    "blend_data = []\n",
    "for d in window:\n",
    "    if d in holdings:\n",
    "        h = holdings[d].get(1, {})\n",
    "        blend_data.append({\"date\": d, **{f\"w({c})\": w for c, w in h.items()}})\n",
    "\n",
    "blend_df = pd.DataFrame(blend_data).set_index(\"date\").fillna(0)\n",
    "blend_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Position signal -> contract orders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "example_date = curve_px.index[-1]\n",
    "notional = 1_000_000.0\n",
    "\n",
    "for pos in [1, 5, 10, 20]:\n",
    "    orders = position_to_contract_orders(\n",
    "        holdings, example_date, position=pos, target_notional=-notional\n",
    "    )\n",
    "    print(f\"pos {pos:>2d}: {orders}\")\n",
    "\n",
    "print(f\"\\nDate: {example_date.date()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# As Order dataclasses\n",
    "order_objs = position_to_contract_orders(\n",
    "    holdings, example_date, position=5, target_notional=-notional, as_dataclasses=True\n",
    ")\n",
    "for o in order_objs:\n",
    "    print(o)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Visualise"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Strip curve time series\n",
    "fig, ax = plt.subplots(figsize=(14, 5))\n",
    "for pos in [1, 5, 10, 15, 20]:\n",
    "    ax.plot(curve_px.index, curve_px[pos], label=f\"pos {pos}\", alpha=0.8)\n",
    "ax.set_title(\"SR3 strip curve (real data, smoothstep roll)\")\n",
    "ax.set_ylabel(\"price\")\n",
    "ax.legend(ncol=5)\n",
    "ax.grid(True, alpha=0.3)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Term structure snapshots (evenly spaced)\n",
    "n_snapshots = 6\n",
    "snap_idx = np.linspace(0, len(curve_px) - 1, n_snapshots, dtype=int)\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(10, 5))\n",
    "for i in snap_idx:\n",
    "    d = curve_px.index[i]\n",
    "    ax.plot(\n",
    "        range(1, n_positions + 1),\n",
    "        curve_px.iloc[i].values,\n",
    "        marker=\"o\", markersize=4, label=str(d.date()),\n",
    "    )\n",
    "ax.set_xlabel(\"position\")\n",
    "ax.set_ylabel(\"price\")\n",
    "ax.set_title(\"SR3 term structure snapshots\")\n",
    "ax.legend()\n",
    "ax.grid(True, alpha=0.3)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Implied rate = 100 - price, daily change in bps\nimplied_rate = 100 - curve_px\nrate_chg_bps = (implied_rate.diff() * 100).dropna()\n\nfig, ax = plt.subplots(figsize=(14, 5))\nvbound = rate_chg_bps.values[np.isfinite(rate_chg_bps.values)]\nvlim = np.percentile(np.abs(vbound), 99)\nim = ax.pcolormesh(\n    rate_chg_bps.index, rate_chg_bps.columns, rate_chg_bps.values.T,\n    cmap=\"RdBu_r\", vmin=-vlim, vmax=vlim, shading=\"auto\",\n)\nfig.colorbar(im, ax=ax, label=\"daily rate change (bps)\")\nax.set_ylabel(\"position\")\nax.set_title(\"SR3 strip curve — daily rate changes (bps)\")\nplt.tight_layout()\nplt.show()"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Rate change correlations across positions\ncorr = rate_chg_bps.corr()\n\nfig, ax = plt.subplots(figsize=(8, 7))\nim = ax.imshow(corr.values, cmap=\"RdBu_r\", vmin=corr.values.min(), vmax=1.0)\nax.set_xticks(range(n_positions))\nax.set_xticklabels(range(1, n_positions + 1))\nax.set_yticks(range(n_positions))\nax.set_yticklabels(range(1, n_positions + 1))\nax.set_title(\"Rate change correlations (bps)\")\nfig.colorbar(im, ax=ax)\nplt.tight_layout()\nplt.show()"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compare roll weight functions\n",
    "u = np.linspace(0, 1, 200)\n",
    "smoothstep = 3 * u**2 - 2 * u**3\n",
    "linear = u\n",
    "k = 10.0\n",
    "logistic_raw = 1 / (1 + np.exp(-k * (u - 0.5)))\n",
    "v0 = 1 / (1 + np.exp(-k * (-0.5)))\n",
    "v1 = 1 / (1 + np.exp(-k * (0.5)))\n",
    "logistic = np.clip((logistic_raw - v0) / (v1 - v0), 0, 1)\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(8, 4))\n",
    "ax.plot(u, linear, label=\"linear\")\n",
    "ax.plot(u, smoothstep, label=\"smoothstep\", linewidth=2)\n",
    "ax.plot(u, logistic, label=\"logistic (k=10)\", linestyle=\"--\")\n",
    "ax.set_xlabel(\"roll progress (u)\")\n",
    "ax.set_ylabel(\"weight on next contract\")\n",
    "ax.set_title(\"Roll weight functions\")\n",
    "ax.legend()\n",
    "ax.grid(True, alpha=0.3)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cleanup\n",
    "\n",
    "Delete the downloaded parquet to save local space."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if os.path.exists(PARQUET_FILE):\n",
    "    os.remove(PARQUET_FILE)\n",
    "    print(f\"Deleted {PARQUET_FILE}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}